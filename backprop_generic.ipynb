{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction: \n",
    "    def __init__(self, func, derivative): \n",
    "        self.func = func\n",
    "        self.derivative = derivative\n",
    "\n",
    "    def __call__(self, x): \n",
    "        return self.func(x) \n",
    "    \n",
    "class LossFunction(ActivationFunction): \n",
    "    def __call__(self, y, y_hat): \n",
    "        return self.func(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "sigmoid = ActivationFunction(\n",
    "    func=lambda x: _sigmoid(x),\n",
    "    derivative=lambda x: _sigmoid(x) * (1 - _sigmoid(x))\n",
    ")\n",
    "relu = ActivationFunction(\n",
    "    func=lambda x: np.maximum(0, x),\n",
    "    derivative=lambda x: np.where(x > 0, 1, 0)\n",
    ")\n",
    "tanh = ActivationFunction(\n",
    "    func=lambda x: np.tanh(x),\n",
    "    derivative=lambda x: 1 - np.tanh(x) ** 2\n",
    ")\n",
    "mse = LossFunction(\n",
    "    func=lambda y_true, y_pred: np.mean((y_true - y_pred) ** 2), # The function is the average of the squared differences\n",
    "    derivative=lambda y_true, y_pred: 2 * (y_pred - y_true) / y_true.size # The derivative is the negative of the average of the differences times 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_output, activation):\n",
    "        self.n_input = n_input # The number of input nodes\n",
    "        self.n_output = n_output # The number of output nodes\n",
    "        self.activation = activation # The activation function\n",
    "        self.W = np.random.randn(n_input, n_output) # The weight matrix\n",
    "        self.b = np.random.randn(n_output) # The bias vector\n",
    "        self.Z = None # The input to the layer\n",
    "        self.A = None # The output of the layer\n",
    "        self.E = None # The error of the layer\n",
    "        self.dZ = None # The derivative of the error of the layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = X.dot(self.W) + self.b # Compute the input to the layer\n",
    "        self.A = self.activation(self.Z) # Compute the output of the layer\n",
    "        return self.A # Return the output\n",
    "\n",
    "    def backward(self, E):\n",
    "        self.E = E # Assign the propagated error to the layer's error\n",
    "        self.dZ = self.E * self.activation.derivative(self.Z) # Compute the derivative of the error of the layer and assign it to self.dZ\n",
    "        return self.dZ # Return the derivative\n",
    "\n",
    "    def update(self, X, alpha):\n",
    "        self.W -= alpha * np.dot(X.T, self.dZ) # Update the weight matrix\n",
    "        self.b -= alpha * np.sum(self.dZ, axis=0) # Update the bias vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, loss):\n",
    "        self.layers = layers # The list of layers\n",
    "        self.loss = loss # The loss function\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = X # Initialize the output as the input\n",
    "        for layer in self.layers: # For each layer in the network\n",
    "            A = layer.forward(A) # Forward propagate through the layer and update the output\n",
    "        return A # Return the final output\n",
    "\n",
    "    def backward(self, y):\n",
    "        E = self.loss.derivative(y, self.layers[-1].A) # Compute the error at the output layer using the loss function derivative\n",
    "        for i in reversed(range(len(self.layers))): # For each layer in reverse order\n",
    "            E = self.layers[i].backward(E) # Backward propagate through the layer and update the error\n",
    "            if i != 0: # Except for the input layer\n",
    "                E = np.dot(E, self.layers[i].W.T) # propagate the error to the next layer (in reverse order)\n",
    "\n",
    "    def update(self, X, alpha):\n",
    "        for layer in self.layers: # For each layer in the network\n",
    "            layer.update(X, alpha) # Update the weights and biases of the layer using the output of the previous layer\n",
    "            X = layer.A # Update the input for the next layer\n",
    "\n",
    "    def train(self, X, y, alpha, epochs):\n",
    "        for epoch in range(epochs): # For each epoch\n",
    "            A = self.forward(X) # Forward propagate through the network and get the output\n",
    "            E = self.loss(y, A) # Compute the error at the output using the loss function\n",
    "            self.backward(y) # Backward propagate through the network and update the derivatives\n",
    "            self.update(X, alpha) # Update the weights and biases of the network\n",
    "            if epoch % 10000 == 0: # Print the error every 1000 epochs\n",
    "                print(f\"Epoch {epoch}, Error: {np.mean(np.abs(E))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2964425942.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[98], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    Layer(n_input=24, n_output=24)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Define a simple XOR network with two hidden layers and sigmoid activation function\n",
    "net = Network(\n",
    "    layers=[\n",
    "        Layer(n_input=2, n_output=12, activation=relu),\n",
    "        Layer(n_input=12, n_output=24, activation=relu),\n",
    "        Layer(n_input=24, n_output=24, activation=relu),\n",
    "        Layer(n_input=24, n_output=12, activation=relu), \n",
    "        Layer(n_input=12, n_output=1, activation=relu)\n",
    "    ],\n",
    "    loss=mse # Use the mean squared error loss function\n",
    ")\n",
    "\n",
    "# Generate random X values between 0 and 10\n",
    "X = np.random.uniform(0, 10, (1000, 2))\n",
    "\n",
    "# Calculate corresponding y values\n",
    "y = 2*X[:, 0] + 3*X[:, 1]\n",
    "\n",
    "# Reshape y to be a column vector\n",
    "y = y.reshape(-1, 1)# Define the input and output data\n",
    "\n",
    "# Train the network using backpropagation\n",
    "net.train(X, y, alpha=0.00001, epochs=100000)\n",
    "\n",
    "# Test the network with the same input data\n",
    "print(\"Output after training:\")\n",
    "print(net.forward(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
